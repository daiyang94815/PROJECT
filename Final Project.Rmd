---
title: "Final Project"
output: html_notebook
---
#Final Project
##By Yang Dai

###Preparation
Load all kinds of libraries:
```{r "setup", include=FALSE}
# set working directory for easy data reading
knitr::opts_knit$set(root.dir = 'F:/Study and Work/UW/Winter 2018/PUBPOL 599 B Computation Thinking for Governance Analytics/Project/data')

library(readxl)
library('tidyverse')
library('DescTools')
library(moments)
library(scales)
library(gmodels)
library(lubridate)
```

###Get the data
The data I'm using here is [road safety data](https://data.gov.uk/dataset/road-accidents-safety-data) in UK 2016.
I also used [UK population data](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/timeseries/gbpop/pop) in my analysis.
I got the UK shapefiles from [here](http://geoportal.statistics.gov.uk/datasets/local-authority-districts-december-2016-full-clipped-boundaries-in-great-britain).
```{r}
#accidents<-read.csv('https://github.com/daiyang94815/Project/raw/master/data/dftRoadSafety_Accidents_2016.csv')
#casualty<-read.csv('https://github.com/daiyang94815/Project/raw/master/data/Cas.csv')
#vehicle<-read.csv('https://github.com/daiyang94815/Project/raw/master/data/Veh.csv')
#temp = tempfile(fileext = ".xlsx") # use always with Excel
#dataURL <- "https://github.com/daiyang94815/Project/raw/master/data/Road%20Accident%20Safety%20Data%20Guide.xls" # link to data
#download.file(dataURL, destfile=temp, mode='wb')  # file will be downloaded temporarily
#guide_lad = read_excel(temp, sheet =6)
#temp = tempfile(fileext = ".xlsx") # use always with Excel
#dataURL <- "https://github.com/daiyang94815/Project/raw/master/data/ukmidyearpopest2016.xls" # link to data
#download.file(dataURL, destfile=temp, mode='wb')  # file will be downloaded temporarily
#pop = read_excel(temp, sheet =5,skip=4)

accidents<-read.csv('dftRoadSafety_Accidents_2016.csv',stringsAsFactors = F)
casualty<-read.csv('Cas.csv',stringsAsFactors = F)
vehicle<-read.csv('Veh.csv',stringsAsFactors = F)
guide_lad<-read_excel('Road Accident Safety Data Guide.xls', sheet=6)
pop<-read_excel('ukmidyearpopest2016.xls',sheet=5,skip=4)
```

Duplicate the dataset for different uses:
```{r}
accidents_EDA=accidents
accidents_map=accidents
accidents_reg=accidents
vehicle_reg=vehicle
```

The data has these columns:
```{r}
str(accidents_EDA)
```

###Explore the distribution of where accidents_EDA took place (rural/urban) (dichotomous data)
```{r}
#distribution of its values:
table(accidents_EDA$Urban_or_Rural_Area)
```
According to the guide, 1=Urban, 2=Rural, 3=Unallocated. So let's drop the unallocated incidents and label the codes.
```{r}
accidents_EDA[accidents_EDA$Urban_or_Rural_Area==3,]$Urban_or_Rural_Area=NA
accidents_EDA$Urban_or_Rural_Area=factor(as.factor(accidents_EDA$Urban_or_Rural_Area),
                                     labels=c('Urban','Rural'))
                             
```
Re-test:
```{r}
table(accidents_EDA$Urban_or_Rural_Area)
prop.table(table(accidents_EDA$Urban_or_Rural_Area))
```

####Central measurement and dispersion
Make a function for getting mode:
```{r}
getMode=function(aColumn){
  freqTable=table(aColumn)
  maxFrequency=max(freqTable)
  names(freqTable[freqTable==maxFrequency])
}
```
Get the mode to see where accidents_EDA took place more often.
```{r}
getMode(accidents_EDA$Urban_or_Rural_Area)
```

###Explore the distribution of accident severity (ordinal categorical variable)
Label the code:
```{r}
# getting original levels:
levelCat=names(table(accidents_EDA$Accident_Severity))

# reordering original levels:
levelCat=c(rev(levelCat))

# format this into an ordinal variable:
accidents_EDA$Accident_Severity=factor(accidents_EDA$Accident_Severity,
                             levels = levelCat,
                             labels=c('Slight','Serious','Fatal'),ordered=T)
table(accidents_EDA$Accident_Severity)
```

####Central Value
To see which severity happens more often:
```{r,eval=F}
#get the mode
getMode(accidents_EDA$Accident_Severity)
#get the median
Median(accidents_EDA$Accident_Severity,na.rm = T) 
#or median(as.numeric(accidents_EDA$Accident_Severity),na.rm = T)
cumsum(prop.table(table(accidents_EDA$Accident_Severity)))  
```

####Dispersion
```{r,eval=F}
Gini(table(accidents_EDA$Accident_Severity))
```
Gini tells that there is some concentration. The plot should help us get a better idea:
```{r,eval=F}
accidents_EDA[!is.na(accidents_EDA$Accident_Severity),]%>%
  ggplot(aes(Accident_Severity))+geom_bar()
```
###Explore the distribution of month and hour (ordinal categorical variable)
```{r}
library(tidyverse)
accidents_EDA_T=transmute(accidents_EDA,hour=hour(hm(accidents_EDA$Time)),month=month(accidents_EDA$Date))
```

####Central Value:
Let's get the mode:
```{r}
# Applying our function:
getMode(accidents_EDA_T$hour)
getMode(accidents_EDA_T$month)
```

####Dispersion
Let's see the Gini's:
```{r}
Gini(table(accidents_EDA_T$hour))
Gini(table(accidents_EDA_T$month))
```
Gini tells that accidents_EDA happaned roughly equally through 12 months, but have some concentration during a day. The plot should help us get a better idea:
```{r}
data=accidents_EDA_T[!is.na(accidents_EDA_T$month),]
c = ggplot(data,aes(as.factor(month)))
c + geom_bar()
```

```{r}
data=accidents_EDA_T[!is.na(accidents_EDA_T$hour),]
c = ggplot(data,aes(as.factor(hour)))
c + geom_bar()
```
We can see most accidents_EDA happened during morning peak (8am) and afternoon peak (3pm-6pm).

###Explore number of vehicles involved in accidents_EDA (counts)
####Centrality
```{r}
summary(accidents_EDA$Number_of_Vehicles)
```

####Skewness
```{r,eval=F}
skewness(accidents_EDA$Number_of_Vehicles,na.rm=T)
```

####Kurtosis
```{r,eval=F}
kurtosis(accidents_EDA$Number_of_Vehicles,na.rm=T)
```

```{r,eval=F}
accidents_EDA%>%
  ggplot(aes(Number_of_Vehicles))+geom_histogram(bins = 50)+
  theme(axis.text.x = element_text(angle = 60,hjust = 1,size = 5))+
  scale_x_continuous(labels = comma)
```
We can see most accidents_EDA involved 2 vehicles.

###Explore the relationship between Road Surface Conditions and Accident Severity (Categorical - Categorical)
```{r,eval=F}
table(accidents_EDA$Road_Surface_Conditions)
accidents_EDA$Road_Surface_Conditions=factor(as.factor(accidents_EDA$Road_Surface_Conditions),
                             labels=c('Data missing or out of range','Dry','Wet or damp','Snow',
                                      'Frost or ice','Flood over 3cm. deep'))
```
contingency table (crosstab):
```{r,eval=F}
table(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity)
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=F, prop.c=F,prop.chisq=F)
```

ignore data that is missing or out of range in Road_Surface_Conditions:
```{r,eval=F}
levels(accidents_EDA$Road_Surface_Conditions)[1]=NA
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=T, prop.r=F, prop.c=F,prop.chisq=F)
```

compute percents (relative values) marginally:
```{r,eval=F}
# severity by road surface condition:
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=T, prop.c=F,prop.chisq=F)
# road surface condition by severity:
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=F, prop.c=T,prop.chisq=F)
```

test the Ho that the variables are independent (not associated):
```{r,eval=F}
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=F, prop.c=F,prop.chisq=F,chisq=T)
```

visual representation:
```{r,eval=F}
legendPlot=levels(as.factor(unique(accidents_EDA$Accident_Severity)))
bartable = table(accidents_EDA$Accident_Severity,accidents_EDA$Road_Surface_Conditions)  ## get the cross tab
barplot(bartable, beside = TRUE,legend=legendPlot)  ## plot
```

represent the cross table in a nicer way:
```{r,eval=F}
#turn table into a data frame:
accidents_EDA_Tb=as.data.frame(table(accidents_EDA$Accident_Severity,accidents_EDA$Road_Surface_Conditions))
names(accidents_EDA_Tb)=c('Accident_Severity','Road_Surface_Conditions','freq')

#Plot the Data
accidents_EDA_Tb%>%
  ggplot(aes(Accident_Severity,Road_Surface_Conditions )) + theme_bw()+ #white background
  geom_point(aes(size = freq), colour = "green")+ #green dot sized by frequency
  geom_text(aes(label = freq))+ # frequency value as label
  theme(legend.position="none")+ # no legend
  scale_size_continuous(range=c(5,30))+ # limits of point size
  labs(title="You see association?") # with titles!
```

###What attributes may increase the chance that an accident is serious (rather than slight)? (Logistic Regression)
I only want to know how the attributes affect the probability increase from slight to serious, so I need to drop those fatal incidents, and label 3 as Slight, 2 as Serious:
```{r}
accidents_reg=accidents[accidents$Accident_Severity!=1,]
```

```{r}
x=1
for (x in 1:length(accidents_reg$Accident_Severity)) {
  if(accidents_reg[x,'Accident_Severity']==3)accidents_reg[x,'Accident_Severity']=1
}
```
```{r}
accidents_reg$Accident_Severity=accidents_reg$Accident_Severity-2
accidents_reg$Accident_Severity=as.factor(accidents_reg$Accident_Severity)
```


```{r}
#accidents_reg[,'Accident_Severity']=as.data.frame(lapply(accidents_reg[,'Accident_Severity'],                        function(x)ifelse(x==3,1,x)))
```

```{r}
barplot(table(accidents_reg$Accident_Severity))
```
The linear regression model computed the coefficients of X so that we predict the value of Y, being those continuous variables. In logistic regression, we have only two values, 'Slight' and 'Serious' in Y. Then, the model will instead help you see which of the X variables will increase the 'odds' of getting a 'Serious'.

Some data cleaning and formatting before regression:
```{r}
vehicle_reg=dplyr::select(vehicle,Accident_Index,Sex_of_Driver,Age_of_Driver,Age_of_Vehicle)
vehicle_reg=vehicle_reg[vehicle_reg$Sex_of_Driver%in%c(1,2)&vehicle_reg$Age_of_Driver!=-1&vehicle_reg$Age_of_Vehicle!=-1,] #include only Male and Female and valid age
vehicle_reg$Sex_of_Driver=vehicle_reg$Sex_of_Driver-1 #recode Male as 0, Female as 1
```

```{r}
accidents_sex=aggregate(list(Avg_Sex_of_Driver=vehicle_reg$Sex_of_Driver),list(Accident_Index=vehicle_reg$Accident_Index),mean)
accidents_driverAge=aggregate(list(Avg_Age_of_Driver=vehicle_reg$Age_of_Driver),list(Accident_Index=vehicle_reg$Accident_Index),mean)
accidents_vehicleAge=aggregate(list(Avg_Age_of_Vehicle=vehicle_reg$Age_of_Vehicle),list(Accident_Index=vehicle_reg$Accident_Index),mean)
```

```{r}
accidents_reg=merge(accidents_reg,accidents_sex)
merge=merge(accidents_driverAge,accidents_vehicleAge)
accidents_reg=merge(accidents_reg,merge)
accidents_reg=dplyr::select(accidents_reg,Accident_Index,Accident_Severity,Avg_Sex_of_Driver,Avg_Age_of_Driver,Avg_Age_of_Vehicle)
```

The way to request this model is very similar to linear regression:
```{r}
# function 'glm' !
LogitSev_a =glm(Accident_Severity ~ Avg_Sex_of_Driver + Avg_Age_of_Driver + Avg_Age_of_Vehicle, 
                   data = accidents_reg,
                   family = binomial())

# see full results: summary(LogitSev_a)

# see relevant info on coefficients:
results_a=coef(summary(LogitSev_a))
data.frame(CoefficientExp=exp(results_a[,1]),Significant=results_a[,4]<0.05)
```

Instead of the Adjusted RSquared, the GLM function offers the Akaike Information Criterion (AIC) as a relative measure of fitness. If you had two models, the smaller the AIC signals the best one of the two compared. Let's make another model:
```{r}
# remember that presscat is factor
LogitSev_b=glm(Accident_Severity ~ Avg_Sex_of_Driver,data = accidents_reg,
                   family = binomial())
results_b=coef(summary(LogitSev_b))
data.frame(CoefficientExp=exp(results_b[,1]),Significant=results_b[,4]<0.05)
```
Now use the AIC:
```{r}
if (LogitSev_a$aic < LogitSev_b$aic){
    print("model 'a' is better")
}else{print("model 'b' is better")}
```
-How much is each variable in X influencing Y?
As you see above, I have exponentiated the resulting coefficients from the model. That is the first step to give an adequate interpretation. From there, you know there is a direct effect (increases the odds of Y=1) if the coefficient is greater than one, and and inverse effect if less than one, while its closeness to 1 would mean no effect. As before, we should be confident of a coefficient value if this were significant.
Keep in mind that if the coefficient is categorical, the increase or decrease of the odds depends on what level of the category is the reference. In the case above, presscatLow decreases the odds respect to presscatHigh.
-Do I have a good logistic model?
Logistics regression can have similar problems that need to be evaluated as in the case of linear regression. However, the critical way to compare the output is the **confusion matrix**.
The confusion matrix requires two inputs, the actual values and the values predicted by the model:
```{r}
actualValues=accidents_reg$Accident_Severity
predictedValues=as.numeric(predict(LogitSev_a, type = 'response'))
```
Using the values above:
```{r}
library(InformationValue)

cm=confusionMatrix(actualValues, predictedValues)
row.names(cm)=c('PredictedNegative','PredictedPositive')
colnames(cm)=c('ActualNegative','ActualPositive')
cm
```
From the table above, we know that the model predicted well 47 zeros and 51 ones, thats is 98 matches out of 129. So, we got 31 out out 129 mistakes, which is interpreted as the misclassification error rate:
```{r}
misClassError(actualValues, predictedValues)
```
Another key concept in evaluating this model is the ROC curve:
```{r}
plotROC(actualValues, predictedValues)
```
The better the model did, the more this curve will expand towards the left top corner. You can also see the value of the area under the curve (AUROC). The most the AUROC can be be is 1. The model is worthless if AUROC is 0.5, when the border of the curve is close to the diagonal (from bottom left to top rigth).
Notice the labels sensitivity and specificity. These are important concepts that can be used in the logistic regression context. They are build upon the following concepts:
```{r}
TruePositive=cm['PredictedPositive','ActualPositive']
TrueNegative=cm['PredictedNegative','ActualNegative']
FalsePositive=cm['PredictedPositive','ActualNegative']
FalseNegative=cm['PredictedNegative','ActualPositive']
```
Sensitivity is the true positive rate (TPR), that is, the probability of a positive prediction given that the case was positive.
```{r}
# TruePositive/(TruePositive+FalseNegative)
sensitivity(actualValues, predictedValues)
```
Specificity is the true negative rate (TNR), that is, the probability of a negative prediction given that the case was negative.
```{r}
# TrueNegative/(FalsePositive+TrueNegative)
specificity(actualValues, predictedValues)
```
The interpretation of these values needs to be contextualized according to the decision-making situation.

###Mapping Accident Severity
Because they coded 1 as Fatal, 3 as Slight, I want them to be reversed to reflect common sense (higher score means more dangerous).
```{r}
library(psych)
accidents_map$Accident_Severity=reverse.code(-1,accidents_map$Accident_Severity)
```

Because one has to be 17 or above to legally drive in UK, I sum the popluation whose age is 17 or above.
```{r}
popabove17<-as.data.frame(apply(pop[,c(21:94)],1,sum))
popabove17<-transmute(popabove17,Code=pop$Code,Name=pop$Name,Pop=apply(pop[, c(21:94)], 1, sum))
```

####Getting the Map
Get the map from GitHub:
```{r}
#compressedMap= "https://github.com/daiyang94815/Project/raw/master/data/Local_Authority_Districts_December_2016_Full_Clipped_Boundaries_in_Great_Britain.zip"
```
```{r}
#library(utils)
#temp=tempfile()
#download.file(compressedMap, temp)
#unzip(temp)
```
To know what shapefiles are now in your directory:
```{r}
(maps=list.files(pattern = 'shp'))
```
You select which map from the object maps you need:
```{r}
library(rgdal)
GBMap <- rgdal::readOGR("Local_Authority_Districts_December_2016_Full_Clipped_Boundaries_in_Great_Britain.shp",stringsAsFactors=F) # use name
```
Now that you have a map, you can use common commands and see what you have:
```{r}
names(GBMap)
```
The Local Authority Districts (LAD) are coded as numbers in the dataset. I want to assign them the names according to the guide.
```{r}
accidents_map=merge(accidents_map,guide_lad,by.x='Local_Authority_.District.',by.y='code',all.x=T)
accidents_map=rename(accidents_map,lab_lad=label)
```
I want to know which LADs are dangerous (have higher total accident-severity score). So I aggregate the accident severity to the LAD level.
```{r}
accidents_lad=aggregate(accidents_map$Accident_Severity,list(accidents_map$lab_lad),sum)
names(accidents_lad)=c('Local_Authority_.District.','Accident_Severity')
```
Then I merge the accident date and population data, then send it to the map
```{r}
accidentsGBmap=merge(GBMap,accidents_lad, by.x='lad16nm', by.y='Local_Authority_.District.',all=T)
accidentsGBmap=merge(accidentsGBmap,popabove17, by.x='lad16nm', by.y='Name',all=T)
```
Total accident severity is not good, so I calculate the ratio (severity to population).
```{r}
accidentsGBmap$AcciDen=accidentsGBmap$Accident_Severity/accidentsGBmap$Pop
```
Let's explore the ratio in each LAD:
```{r}
summary(accidentsGBmap$AcciDen)
```
We got 8 NAs,
```{r}
# notice the use of '@data'
#finding the issue:
accidentsGBmap@data[is.na(accidentsGBmap$AcciDen),]['AcciDen']
```
We need to get rid of those rows:
```{r}
accidentsGBmap=accidentsGBmap[!is.na(accidentsGBmap$AcciDen),]
```
That has solved the problem:
```{r}
summary(accidentsGBmap$AcciDen)
```
We will plot the new variable asking for 5 quantiles. Let's follow these steps:
1.Install and load the necessary packages to manage color and divisions:
```{r}
library(RColorBrewer)
library(classInt)
```
2.Define the input:
```{r}
varToPLot=accidentsGBmap$AcciDen
```
3.Get colors and intervals (you can choose palettes from here):
```{r}
numberOfClasses = 3
colorForScale='YlGnBu'
colors = brewer.pal(numberOfClasses, colorForScale)
intervals <- classIntervals(varToPLot, numberOfClasses, 
                            style = "quantile",
                            dataPrecision=5)
colorPallette <- findColours(intervals, colors)
```
4.Plot
```{r}
legendText="Accident Severity Rate"
shrinkLegend=0.5
title="Accident Severity Rate in UK 2016"

plot(accidentsGBmap, col = colorPallette,border='grey',main=title,add=F)

legend('topright', legend = names(attr(colorPallette, "table")), 
       fill = attr(colorPallette, "palette"), cex = shrinkLegend, 
       bty = "n",
       title=legendText)
```
