---
title: "Final Project"
output: html_notebook
---
#Final Project
##By Yang Dai

###Preparation
Load all kinds of libraries:
```{r "setup", include=FALSE}
# set working directory for easy data reading
knitr::opts_knit$set(root.dir = 'F:/Study and Work/UW/Winter 2018/PUBPOL 599 B Computation Thinking for Governance Analytics/Project/data')

library(readxl)
library('tidyverse')
library('DescTools')
library(moments)
library(scales)
library(gmodels)
library(lubridate)
```

###Get the data
The data I'm using here is [road safety data](https://data.gov.uk/dataset/road-accidents-safety-data) in UK 2016. There are 3 datasets: Accidents, Vehicle, Casualty. As it's easy to understand, an accident is mostly likely involve more than 1 vehicle, and it's likely to have multiple casualties from different vehicles. So Vehicle and Casualty are linked with Accidents by Accident_Index, Casualty and Vehicle are linked by Vehicle_Reference.

I also used [UK population data](https://www.ons.gov.uk/peoplepopulationandcommunity/populationandmigration/populationestimates/timeseries/gbpop/pop) in my analysis.

I got the UK shapefiles from [here](http://geoportal.statistics.gov.uk/datasets/local-authority-districts-december-2016-full-clipped-boundaries-in-great-britain).
```{r}
#accidents<-read.csv('https://github.com/daiyang94815/Project/raw/master/data/dftRoadSafety_Accidents_2016.csv')
#vehicle<-read.csv('https://github.com/daiyang94815/Project/raw/master/data/Veh.csv')
#casualty<-read.csv('https://github.com/daiyang94815/Project/raw/master/data/Cas.csv')
#temp = tempfile(fileext = ".xlsx") # use always with Excel
#dataURL <- "https://github.com/daiyang94815/Project/raw/master/data/Road%20Accident%20Safety%20Data%20Guide.xls" # link to data
#download.file(dataURL, destfile=temp, mode='wb')  # file will be downloaded temporarily
#guide_lad = read_excel(temp, sheet =6)
#temp = tempfile(fileext = ".xlsx") # use always with Excel
#dataURL <- "https://github.com/daiyang94815/Project/raw/master/data/ukmidyearpopest2016.xls" # link to data
#download.file(dataURL, destfile=temp, mode='wb')  # file will be downloaded temporarily
#pop = read_excel(temp, sheet =5,skip=4)

accidents<-read.csv('dftRoadSafety_Accidents_2016.csv',stringsAsFactors = F)
vehicle<-read.csv('Veh.csv',stringsAsFactors = F)
casualty<-read.csv('Cas.csv',stringsAsFactors = F)
guide_lad<-read_excel('Road Accident Safety Data Guide.xls', sheet=6)
pop<-read_excel('ukmidyearpopest2016.xls',sheet=5,skip=4)
```

Duplicate the dataset for different uses:
```{r}
accidents_EDA=accidents
accidents_map=accidents
accidents_reg=accidents
vehicle_reg=vehicle
```

The data has these columns:
```{r}
str(accidents_EDA)
```

###Explore the distribution of where accidents_EDA took place (rural/urban) (dichotomous data)
```{r}
#distribution of its values:
table(accidents_EDA$Urban_or_Rural_Area)
```
According to the guide, 1=Urban, 2=Rural, 3=Unallocated. So let's drop the unallocated incidents and label the codes.
```{r}
accidents_EDA[accidents_EDA$Urban_or_Rural_Area==3,]$Urban_or_Rural_Area=NA
accidents_EDA$Urban_or_Rural_Area=factor(as.factor(accidents_EDA$Urban_or_Rural_Area),
                                     labels=c('Urban','Rural'))
                             
```
Re-test:
```{r}
table(accidents_EDA$Urban_or_Rural_Area)
prop.table(table(accidents_EDA$Urban_or_Rural_Area))
```
Apparently, accidents happen more often in urban areas.

####Central measurement and dispersion
Make a function for getting mode:
```{r}
getMode=function(aColumn){
  freqTable=table(aColumn)
  maxFrequency=max(freqTable)
  names(freqTable[freqTable==maxFrequency])
}
```
Get the mode to see where accidents_EDA took place more often.
```{r}
getMode(accidents_EDA$Urban_or_Rural_Area)
```

###Explore the distribution of accident severity (ordinal categorical variable)
Label the code:
```{r}
# getting original levels:
levelCat=names(table(accidents_EDA$Accident_Severity))

# reordering original levels:
levelCat=c(rev(levelCat))

# format this into an ordinal variable:
accidents_EDA$Accident_Severity=factor(accidents_EDA$Accident_Severity,
                             levels = levelCat,
                             labels=c('Slight','Serious','Fatal'),ordered=T)
```
Then see the distribution:
```{r}
table(accidents_EDA$Accident_Severity)
```

####Central Value
To see which severity happens more often:
```{r,eval=F}
#get the mode
getMode(accidents_EDA$Accident_Severity)
#get the median
Median(accidents_EDA$Accident_Severity,na.rm = T) 
#or median(as.numeric(accidents_EDA$Accident_Severity),na.rm = T)
cumsum(prop.table(table(accidents_EDA$Accident_Severity)))  
```
Most of the accidents happened are slight.

####Dispersion
```{r,eval=F}
Gini(table(accidents_EDA$Accident_Severity))
```
Gini tells that there is some concentration. The plot should help us get a better idea:
```{r,eval=F}
accidents_EDA[!is.na(accidents_EDA$Accident_Severity),]%>%
  ggplot(aes(Accident_Severity))+geom_bar()
```

###Explore the distribution of month and hour (ordinal categorical variable)
Now I want to know how accidents happen in the year and in a day.
Here I create 2 new variables of hour and month.
```{r}
library(tidyverse)
accidents_EDA_T=transmute(accidents_EDA,hour=hour(hm(accidents_EDA$Time)),month=month(accidents_EDA$Date))
```

####Central Value:
Let's see in what hour and what month did accidents happen most:
```{r}
# Applying our function:
getMode(accidents_EDA_T$hour)
getMode(accidents_EDA_T$month)
```

####Dispersion
Let's see the Gini's:
```{r}
Gini(table(accidents_EDA_T$month))
Gini(table(accidents_EDA_T$hour))
```
Gini tells that accidents_EDA happaned roughly equally through 12 months, but have some concentration during a day. The plot should help us get a better idea:
```{r}
data=accidents_EDA_T[!is.na(accidents_EDA_T$month),]
c = ggplot(data,aes(as.factor(month)))
c + geom_bar()
```

```{r}
data=accidents_EDA_T[!is.na(accidents_EDA_T$hour),]
c = ggplot(data,aes(as.factor(hour)))
c + geom_bar()
```
We can see most accidents_EDA happened during morning peak (8am) and afternoon peak (3pm-6pm).

###Explore number of vehicles involved in accidents_EDA (counts)
####Centrality
Now I want to know usually how many vehicles are involved in an accident:
```{r}
summary(accidents_EDA$Number_of_Vehicles)
```
We can see most accidents involve 2 vehicles, but there is 1 major accidents involving 16 vehicles!

####Skewness
```{r,eval=F}
skewness(accidents_EDA$Number_of_Vehicles,na.rm=T)
```
Positive value indicates the distribution is right-skewed, meaning most accidents involve few vehicles.

####Kurtosis
```{r,eval=F}
kurtosis(accidents_EDA$Number_of_Vehicles,na.rm=T)
```
Positive value indicates it's more concentrated than a normal distribution (the mode is very siginificant.)

```{r,eval=F}
accidents_EDA%>%
  ggplot(aes(Number_of_Vehicles))+geom_histogram(bins = 30)+
  theme(axis.text.x = element_text(angle = 0,hjust = 0,size = 8))+
  scale_x_continuous(labels = comma)
```
We can see most accidents involved 2 vehicles.

###Explore the relationship between Road Surface Conditions and Accident Severity (Categorical - Categorical)
Now I want to know what is the relationship between road surface conditions and accident severity.
Let's label the road surface conditions first.
```{r,eval=F}
accidents_EDA$Road_Surface_Conditions=factor(as.factor(accidents_EDA$Road_Surface_Conditions),
                             labels=c('Data missing or out of range','Dry','Wet or damp','Snow',
                                      'Frost or ice','Flood over 3cm. deep'))
```
Then explore the distribution:
```{r}
table(accidents_EDA$Road_Surface_Conditions)
```

better to see with a contingency table (crosstab):
```{r,eval=F}
table(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity)
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=F, prop.c=F,prop.chisq=F)
```

ignore data that is missing or out of range in Road_Surface_Conditions:
```{r,eval=F}
levels(accidents_EDA$Road_Surface_Conditions)[1]=NA
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=T, prop.r=F, prop.c=F,prop.chisq=F)
```

compute percents (relative values) marginally:
```{r,eval=F}
# severity by road surface condition:
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=T, prop.c=F,prop.chisq=F)
# road surface condition by severity:
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=F, prop.c=T,prop.chisq=F)
```
I think the data on "Snow","Frost or ice","Flood over 3cm. deep" is too few to draw some conclusion, but we can see there's slightly higher portion of Serious accidents on Wet or damp roads than on Dry road.
Let's test the Ho that the variables are independent (not associated/different in portion of accident severity):
```{r,eval=F}
CrossTable(accidents_EDA$Road_Surface_Conditions,accidents_EDA$Accident_Severity,prop.t=F, prop.r=F, prop.c=F,prop.chisq=F,chisq=T)
```
Chi-squared test reject the null hypothesis that there is no difference in accident severity with different road surface conditions.

Let's create the visual representation:
```{r,eval=F}
legendPlot=levels(as.factor(unique(accidents_EDA$Accident_Severity)))
bartable = table(accidents_EDA$Accident_Severity,accidents_EDA$Road_Surface_Conditions)  ## get the cross tab
barplot(bartable, beside = TRUE,legend=legendPlot)  ## plot
```

represent the cross table in a nicer way:
```{r,eval=F}
#turn table into a data frame:
accidents_EDA_Tb=as.data.frame(table(accidents_EDA$Accident_Severity,accidents_EDA$Road_Surface_Conditions))
names(accidents_EDA_Tb)=c('Accident_Severity','Road_Surface_Conditions','freq')

#Plot the Data
accidents_EDA_Tb%>%
  ggplot(aes(Accident_Severity,Road_Surface_Conditions )) + theme_bw()+ #white background
  geom_point(aes(size = freq), colour = "green")+ #green dot sized by frequency
  geom_text(aes(label = freq))+ # frequency value as label
  theme(legend.position="none")+ # no legend
  scale_size_continuous(range=c(5,30))+ # limits of point size
  labs(title="You see association?") # with titles!
```

###What attributes may increase the chance that an accident is serious (rather than slight)? (Logistic Regression)
I only want to know how the attributes affect the probability increase from slight to serious, so I need to drop those fatal incidents:
```{r}
accidents_reg=select(accidents,Accident_Index,Accident_Severity)
accidents_reg=accidents_reg[accidents_reg$Accident_Severity!=1,]
```
And recode 0 as Serious, 1 as Slight.
```{r}
accidents_reg$Accident_Severity=accidents_reg$Accident_Severity-2
accidents_reg$Accident_Severity=as.factor(accidents_reg$Accident_Severity)
```
Check the dependent variable (accident severity) is dichotomous.
```{r}
barplot(table(accidents_reg$Accident_Severity))
```
In logistic regression, we have only two values, 0 and 1 ('Serious' and 'Slight') in Y. Then, the model will instead help you see which of the X variables will increase the 'odds' of getting a 1 ('Slight').

Some data cleaning and formatting before regression:
```{r}
vehicle_reg=select(vehicle,Accident_Index,Sex_of_Driver,Age_of_Driver,Age_of_Vehicle)
vehicle_reg=vehicle_reg[vehicle_reg$Sex_of_Driver%in%c(1,2)&vehicle_reg$Age_of_Driver!=-1&vehicle_reg$Age_of_Vehicle!=-1,] #include only Male and Female and valid age
vehicle_reg$Sex_of_Driver=vehicle_reg$Sex_of_Driver-1 #recode Male as 0, Female as 1
```

I averaged Sex of Driver, Age of Driver, Age of Vehicle. I think it's easy to understand why I chose last 2 variables. The logic behind average Sex of Driver lay on how I code them. As you see above, I code Male as 0, Female as 1. Thus, if the average Sex of Driver is close to 1, that means there are more female drivers, and vice versa.
```{r}
accidents_sex=aggregate(list(Avg_Sex_of_Driver=vehicle_reg$Sex_of_Driver),list(Accident_Index=vehicle_reg$Accident_Index),mean)
accidents_driverAge=aggregate(list(Avg_Age_of_Driver=vehicle_reg$Age_of_Driver),list(Accident_Index=vehicle_reg$Accident_Index),mean)
accidents_vehicleAge=aggregate(list(Avg_Age_of_Vehicle=vehicle_reg$Age_of_Vehicle),list(Accident_Index=vehicle_reg$Accident_Index),mean)
```

Merge all the dependent and independent variables into 1 dataframe.
```{r}
accidents_reg=merge(accidents_reg,accidents_sex)
merge=merge(accidents_driverAge,accidents_vehicleAge)
accidents_reg=merge(accidents_reg,merge)
```

The way to request this model is very similar to linear regression:
```{r}
# function 'glm' !
LogitSev_a =glm(Accident_Severity ~ Avg_Sex_of_Driver + Avg_Age_of_Driver + Avg_Age_of_Vehicle, 
                   data = accidents_reg,
                   family = binomial())

# see full results: summary(LogitSev_a)

# see relevant info on coefficients:
results_a=coef(summary(LogitSev_a))
data.frame(CoefficientExp=exp(results_a[,1]),Significant=results_a[,4]<0.05)
```

Instead of the Adjusted RSquared, the GLM function offers the Akaike Information Criterion (AIC) as a relative measure of fitness. If you had two models, the smaller the AIC signals the best one of the two compared. Let's make another model:
```{r}
# remember that presscat is factor
LogitSev_b=glm(Accident_Severity ~ Avg_Sex_of_Driver,data = accidents_reg,
                   family = binomial())
results_b=coef(summary(LogitSev_b))
data.frame(CoefficientExp=exp(results_b[,1]),Significant=results_b[,4]<0.05)
```
Now use the AIC:
```{r}
if (LogitSev_a$aic < LogitSev_b$aic){
    print("model 'a' is better")
}else{print("model 'b' is better")}
```
-How much is each variable in X influencing Y?
As you see above, I have exponentiated the resulting coefficients from the model. That is the first step to give an adequate interpretation. From there, you know there is a direct effect (increases the odds of Y=1) if the coefficient is greater than one, and and inverse effect if less than one, while its closeness to 1 would mean no effect. As before, we should be confident of a coefficient value if this were significant.
Keep in mind that if the coefficient is categorical, the increase or decrease of the odds depends on what level of the category is the reference. In the case above, presscatLow decreases the odds respect to presscatHigh.
-Do I have a good logistic model?
Logistics regression can have similar problems that need to be evaluated as in the case of linear regression. However, the critical way to compare the output is the **confusion matrix**.
The confusion matrix requires two inputs, the actual values and the values predicted by the model:
```{r}
actualValues=accidents_reg$Accident_Severity
predictedValues=predict(LogitSev_a, type = 'response')
```
Using the values above:
```{r}
library(InformationValue)

cm=confusionMatrix(actualValues, predictedValues)
#row.names(cm)=c('PredictedNegative','PredictedPositive')
colnames(cm)=c('ActualNegative','ActualPositive')
cm
```
From the table above, we know that the model predicted well 47 zeros and 51 ones, thats is 98 matches out of 129. So, we got 31 out out 129 mistakes, which is interpreted as the misclassification error rate:
```{r}
misClassError(actualValues, predictedValues)
```
Another key concept in evaluating this model is the ROC curve:
```{r}
plotROC(actualValues, predictedValues)
```
The better the model did, the more this curve will expand towards the left top corner. You can also see the value of the area under the curve (AUROC). The most the AUROC can be be is 1. The model is worthless if AUROC is 0.5, when the border of the curve is close to the diagonal (from bottom left to top rigth).
Notice the labels sensitivity and specificity. These are important concepts that can be used in the logistic regression context. They are build upon the following concepts:
```{r}
TruePositive=cm['PredictedPositive','ActualPositive']
TrueNegative=cm['PredictedNegative','ActualNegative']
FalsePositive=cm['PredictedPositive','ActualNegative']
FalseNegative=cm['PredictedNegative','ActualPositive']
```
Sensitivity is the true positive rate (TPR), that is, the probability of a positive prediction given that the case was positive.
```{r}
# TruePositive/(TruePositive+FalseNegative)
sensitivity(actualValues, predictedValues)
```
Specificity is the true negative rate (TNR), that is, the probability of a negative prediction given that the case was negative.
```{r}
# TrueNegative/(FalsePositive+TrueNegative)
specificity(actualValues, predictedValues)
```
The interpretation of these values needs to be contextualized according to the decision-making situation.

###Mapping Accident Severity
Subset the data first:
```{r}
accidents_map=select(accidents,Accident_Severity,Local_Authority_.District.)
```

In mapping, I want to code the accident severity differently from above. Because they coded 1 as Fatal, 3 as Slight, I want them to be reversed to reflect common sense (higher score means more dangerous).
```{r}
library(psych)
accidents_map$Accident_Severity=reverse.code(-1,accidents_map$Accident_Severity)
```

Because one has to be 17 or above to legally drive in UK, I sum the popluation whose age is 17 or above.
```{r}
popabove17<-as.data.frame(apply(pop[,c(21:94)],1,sum))
popabove17<-transmute(popabove17,Code=pop$Code,Name=pop$Name,Pop=apply(pop[,c(21:94)],1,sum))
```

####Getting the Map
Get the map from GitHub:
```{r}
#compressedMap= "https://github.com/daiyang94815/Project/raw/master/data/Local_Authority_Districts_December_2016_Full_Clipped_Boundaries_in_Great_Britain.zip"
```
```{r}
#library(utils)
#temp=tempfile()
#download.file(compressedMap, temp)
#unzip(temp)
```
To know what shapefiles are now in your directory:
```{r}
(maps=list.files(pattern = 'shp'))
```
You select which map from the object maps you need:
```{r}
library(rgdal)
GBMap <- rgdal::readOGR("Local_Authority_Districts_December_2016_Full_Clipped_Boundaries_in_Great_Britain.shp",stringsAsFactors=F) # use name
```
Now that you have a map, you can use common commands and see what you have:
```{r}
names(GBMap)
```
The Local Authority Districts (LAD) are coded as numbers in the dataset. I want to assign them the names according to the guide.
```{r}
accidents_map=merge(accidents_map,guide_lad,by.x='Local_Authority_.District.',by.y='code',all.x=T)
accidents_map=rename(accidents_map,lab_lad=label)
```
I want to know which LADs are dangerous (have higher total accident severity score). So I aggregate the accident severity to the LAD level. Remember how I code the accident severity, I reversed the original code, so now 1 is Slight, 2 is Serious, and 3 is Fatal. And the aggregation of these value is a rough estimation (accident severity score) of how dangerous a LAD is. This contains a implict (and not necessarily right) assumption that a fatal accident is 2 times more dangerous than a slight one.
```{r}
accidents_lad=aggregate(accidents_map$Accident_Severity,list(accidents_map$lab_lad),sum)
names(accidents_lad)=c('Local_Authority_.District.','Accident_Severity_Score')
```
Then I merge the accident date and population data, then send it to the map
```{r}
accidentsGBmap=merge(GBMap,accidents_lad, by.x='lad16nm', by.y='Local_Authority_.District.',all=T)
accidentsGBmap=merge(accidentsGBmap,popabove17, by.x='lad16nm', by.y='Name',all=T)
```
Total accident severity is not good, so I calculate the ratio (severity to population).
```{r}
accidentsGBmap$AcciDen=accidentsGBmap$Accident_Severity_Score/accidentsGBmap$Pop
```
Let's explore the ratio in each LAD:
```{r}
summary(accidentsGBmap$AcciDen)
```
We got 8 NAs,
```{r}
# notice the use of '@data'
#finding the issue:
accidentsGBmap@data[is.na(accidentsGBmap$AcciDen),]['AcciDen']
```
We need to get rid of those rows:
```{r}
accidentsGBmap=accidentsGBmap[!is.na(accidentsGBmap$AcciDen),]
```
That has solved the problem:
```{r}
summary(accidentsGBmap$AcciDen)
```
We will plot the new variable asking for 5 quantiles. Let's follow these steps:
1.Install and load the necessary packages to manage color and divisions:
```{r}
library(RColorBrewer)
library(classInt)
```
2.Define the input:
```{r}
varToPLot=accidentsGBmap$AcciDen
```
3.Get colors and intervals (you can choose palettes from here):
```{r}
numberOfClasses = 3
colorForScale='YlGnBu'
colors = brewer.pal(numberOfClasses, colorForScale)
intervals <- classIntervals(varToPLot, numberOfClasses, 
                            style = "quantile",
                            dataPrecision=5)
colorPallette <- findColours(intervals, colors)
```
4.Plot
```{r}
legendText="Accident Severity Rate"
shrinkLegend=0.5
title="Accident Severity Rate in UK 2016"

plot(accidentsGBmap, col = colorPallette,border='grey',main=title,add=F)

legend('topright', legend = names(attr(colorPallette, "table")), 
       fill = attr(colorPallette, "palette"), cex = shrinkLegend, 
       bty = "n",
       title=legendText)
```
